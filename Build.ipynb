{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyGithub\n",
      "  Downloading PyGithub-1.54.1-py3-none-any.whl (289 kB)\n",
      "\u001b[K     |████████████████████████████████| 289 kB 22.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/site-packages (2.22.0)\n",
      "Collecting pyjwt<2.0\n",
      "  Downloading PyJWT-1.7.1-py2.py3-none-any.whl (18 kB)\n",
      "Collecting deprecated\n",
      "  Downloading Deprecated-1.2.11-py2.py3-none-any.whl (9.1 kB)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/site-packages (from requests) (1.25.10)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/site-packages (from requests) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/site-packages (from requests) (2020.6.20)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/site-packages (from requests) (2.8)\n",
      "Collecting wrapt<2,>=1.10\n",
      "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
      "Building wheels for collected packages: wrapt\n",
      "  Building wheel for wrapt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wrapt: filename=wrapt-1.12.1-cp36-cp36m-linux_x86_64.whl size=69764 sha256=049d2f5a9b8db3a7ffb2a62139af8880eea2afc094d24e9419aeb5f306f50730\n",
      "  Stored in directory: /home/cdsw/.cache/pip/wheels/32/42/7f/23cae9ff6ef66798d00dc5d659088e57dbba01566f6c60db63\n",
      "Successfully built wrapt\n",
      "Installing collected packages: pyjwt, wrapt, deprecated, PyGithub\n",
      "Successfully installed PyGithub-1.54.1 deprecated-1.2.11 pyjwt-1.7.1 wrapt-1.12.1\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3.6 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install PyGithub requests nb_black\n",
    "!pip3 install ipython --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from github import Github\n",
    "\n",
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "AMP_NAMES = [\n",
    "    \"Churn_Prediction\",\n",
    "    \"Image_Analysis\",\n",
    "    \"Anomaly_Detection\",\n",
    "    \"NeuralQA\",\n",
    "    \"Structural_Time_Series\",\n",
    "    \"SpaCy_Entity_Extraction\",\n",
    "    \"Explainability_LIME_SHAP\",\n",
    "    \"Question_Answering\",\n",
    "    \"Active_Learning\",\n",
    "    \"MLFlow_Tracking\",\n",
    "]\n",
    "\n",
    "AMP_REPOS = [\"cloudera/CML_AMP_\" + amp for amp in AMP_NAMES]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull last 14 days of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_usage_last_14_days(gh_token, amp_repos):\n",
    "    \"\"\"\n",
    "    Provided a GH API token and a list of qualified Github repository names that the\n",
    "    API token has access to, this function pulls usage metrics (clones & views) for\n",
    "    each repository for the last 14 days, and returns as a Pandas DataFrame. It also\n",
    "    collects aggregate metrics on the sources sites driving traffic to each repo\n",
    "    over the last 14 days.\n",
    "\n",
    "    Args:\n",
    "        gh_token (str)\n",
    "        amp_repos List[str]\n",
    "\n",
    "    Returns:\n",
    "        amp_tracking_df (pd.DataFrame)\n",
    "        amp_referring_df (pd_DataFrame)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    gh = Github(gh_token)\n",
    "\n",
    "    activity_dfs = []\n",
    "    referring_dfs = []\n",
    "\n",
    "    for repo in amp_repos:\n",
    "        gh_repo = gh.get_repo(repo)\n",
    "\n",
    "        # gather referring sites as DF\n",
    "        refs = gh_repo.get_top_referrers()\n",
    "        ref_data = []\n",
    "        for ref in refs:\n",
    "            data = {\n",
    "                \"referrer\": ref.referrer,\n",
    "                \"refs_unique\": ref.uniques,\n",
    "                \"refs_total\": ref.count,\n",
    "            }\n",
    "            ref_data.append(data)\n",
    "        ref_df = pd.DataFrame(ref_data)\n",
    "        ref_df[\"repo\"] = repo[17:]\n",
    "        referring_dfs.append(ref_df)\n",
    "\n",
    "        # gather view activity as DF\n",
    "        views = gh_repo.get_views_traffic(per=\"day\")\n",
    "        view_data = []\n",
    "        for view in views[\"views\"]:\n",
    "            data = {\n",
    "                \"timestamp\": view.timestamp,\n",
    "                \"views_unique\": view.uniques,\n",
    "                \"views_total\": view.count,\n",
    "            }\n",
    "            view_data.append(data)\n",
    "        view_df = pd.DataFrame(view_data).set_index(\"timestamp\")\n",
    "        idx = pd.date_range(\n",
    "            end=pd.to_datetime(\"today\").date().strftime(\"%m-%d-%Y\"),\n",
    "            start=(\n",
    "                pd.to_datetime(\"today\").date() - datetime.timedelta(days=14)\n",
    "            ).strftime(\"%m-%d-%Y\"),\n",
    "        )\n",
    "        view_df = view_df.reindex(idx, fill_value=0)\n",
    "\n",
    "        # gather clone activity as DF\n",
    "        clones = gh_repo.get_clones_traffic(per=\"day\")\n",
    "        clone_data = []\n",
    "        for clone in clones[\"clones\"]:\n",
    "            data = {\n",
    "                \"timestamp\": clone.timestamp,\n",
    "                \"clones_unique\": clone.uniques,\n",
    "                \"clones_total\": clone.count,\n",
    "            }\n",
    "            clone_data.append(data)\n",
    "\n",
    "        clone_df = pd.DataFrame(clone_data).set_index(\"timestamp\")\n",
    "        clone_df = clone_df.reindex(idx, fill_value=0)\n",
    "\n",
    "        # combine DFs\n",
    "        activity_df = pd.concat([clone_df, view_df], axis=1)\n",
    "        activity_df[\"repo\"] = repo[17:]\n",
    "\n",
    "        activity_dfs.append(activity_df)\n",
    "\n",
    "    amp_tracking_df = pd.concat(activity_dfs)\n",
    "    amp_referring_df = pd.concat(referring_dfs).reset_index(drop=True)\n",
    "\n",
    "    return amp_tracking_df, amp_referring_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update Live Data\n",
    "\n",
    "Every morning at 8 a.m. EST, a Job will run that:\n",
    "1. Gets the latest 14 day usage stats (both tracking and referring)\n",
    "2. Pulls the 2nd to last day stats for the tracking metrics(basically the final full count of yesterdays stats) and save to daily_archive folder\n",
    "3. Loads yesterday's production tracking_df archive pkl file and appends latest days stats to it\n",
    "4. Saves a pkl of the new cumulative DF and new referring df (in archive folder)\n",
    "5. Deletes the existing tables\n",
    "6. Recreates the tables (this ensures we always have an IDENTICAL backup as a pkl DF)\n",
    "\n",
    "Note that the datetime module is ahead of the Github timezone by ~5 hours, so by running this every morning at 8 a.m. and taking the second to last day, we'll always capture the full prior day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Get latest 14 day usage - both tracking and referring DFs\n",
    "amp_tracking_df, amp_referring_df = get_usage_last_14_days(\n",
    "    gh_token=TOKEN, amp_repos=AMP_REPOS\n",
    ")\n",
    "\n",
    "# 2. Pull just the second to last days stats from tracking DF (basically the final full count of yesterdays stats) and save to daily_archive\n",
    "completed_day = amp_tracking_df.index[-2]\n",
    "daily_tracking_df = amp_tracking_df.loc[completed_day]\n",
    "\n",
    "today_str = datetime.datetime.today().strftime(\"%m-%d-%Y\")\n",
    "yesterday_str = (datetime.datetime.today() - datetime.timedelta(days=1)).strftime(\n",
    "    \"%m-%d-%Y\"\n",
    ")\n",
    "os.makedirs(f\"data/daily_archive/{today_str}\")\n",
    "\n",
    "daily_tracking_df.to_pickle(\n",
    "    f\"data/daily_archive/{today_str}/daily_tracking_{today_str}.pkl\"\n",
    ")\n",
    "\n",
    "amp_referring_df.to_pickle(\n",
    "    f\"data/daily_archive/{today_str}/daily_referring_last14_{today_str}.pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Load yesterday's production tracking_df and append new daily_tracking_df\n",
    "old_prod_tracking = pd.read_pickle(\n",
    "    f\"data/prod_archive/{yesterday_str}/cumulative_tracking_{yesterday_str}.pkl\"\n",
    ")\n",
    "new_prod_tracking = pd.concat([old_prod_tracking, daily_tracking_df])\n",
    "\n",
    "os.makedirs(f\"data/prod_archive/{today_str}\")\n",
    "new_prod_tracking.to_pickle(\n",
    "    f\"data/prod_archive/{today_str}/cumulative_tracking_{today_str}.pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Delete existing SQLite tables\n",
    "conn = create_connection(f\"{os.getcwd()}/db/pythonsqlite.db\")\n",
    "\n",
    "delete_table(\"amp_tracking\", conn)\n",
    "delete_table(\"amp_referring\", conn)\n",
    "\n",
    "# 5. Create new tables to refresh data\n",
    "\n",
    "create_table_from_df(\"amp_tracking\", conn, new_prod_tracking)\n",
    "create_table_from_df(\"amp_referring\", conn, amp_referring_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQLite DB Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from sqlite3 import Error\n",
    "\n",
    "\n",
    "def create_connection(db_file):\n",
    "    \"\"\"\n",
    "    Create a database connection to the SQLite database specified by db_file\n",
    "\n",
    "    Args:\n",
    "        database file\n",
    "\n",
    "    Returns:\n",
    "        Connection object\n",
    "    \"\"\"\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_file)\n",
    "        return conn\n",
    "    except Error as e:\n",
    "        print(e)\n",
    "\n",
    "    return conn\n",
    "\n",
    "\n",
    "def create_table_from_df(table_name, conn, df):\n",
    "    \"\"\"\n",
    "    Create a SQLite table given the name, connection, and\n",
    "    pandas dataframe\n",
    "\n",
    "    \"\"\"\n",
    "    df.to_sql(name=table_name, con=conn)\n",
    "\n",
    "\n",
    "def delete_table(table_name, conn):\n",
    "    \"\"\"\n",
    "    Delete a table in a SQLite DB given the connection and table name\n",
    "\n",
    "    \"\"\"\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    qry = f\"DROP TABLE {table_name}\"\n",
    "    cur.execute(qry)\n",
    "\n",
    "\n",
    "def select_all_from_table(table_name, conn):\n",
    "    \"\"\"\n",
    "    Query a table in a SQLite DB given the connection and table name\n",
    "    for all records and return as pandas dataframe\n",
    "\n",
    "    \"\"\"\n",
    "    df = pd.read_sql_query(f\"SELECT * FROM {table_name}\", conn)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = create_connection(f\"{os.getcwd()}/db/pythonsqlite.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_table_from_df(\"amp_tracking\", conn, amp_tracking_df)\n",
    "create_table_from_df(\"amp_referring\", conn, amp_referring_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x7f2fa6ce7500>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur.execute(\"PRAGMA table_info(amp_tracking)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'index', 'TIMESTAMP', 0, None, 0),\n",
       " (1, 'clones_unique', 'INTEGER', 0, None, 0),\n",
       " (2, 'clones_total', 'INTEGER', 0, None, 0),\n",
       " (3, 'views_unique', 'INTEGER', 0, None, 0),\n",
       " (4, 'views_total', 'INTEGER', 0, None, 0),\n",
       " (5, 'repo', 'TEXT', 0, None, 0)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur.fetchall()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
